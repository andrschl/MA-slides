\documentclass[11pt,aspectratio=169]{beamer}

% my packages and definitions
\usepackage{tikz}
\usepackage[font=scriptsize,labelformat=empty]{caption}

\usetikzlibrary{automata, positioning}
\DeclareMathOperator{\tr}{tr}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\R}{\mathbb{R}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\myvec}[1]{\ensuremath{\begin{pmatrix}#1\end{pmatrix}}}
\newcommand*{\V}[1]{\mathbf{#1}}
\newcommand{\E}{\mathbb{E}}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}

\title{Delayed deep learning for continuous-time dynamical systems}
\date[03.03.21]{Master thesis presentation}
\author{Andreas Schlaginhaufen}
\institute{D-INFK \& D-ITET}

\usetheme{eth}

\colorlet{titlefgcolor}{ETH3}
\colorlet{accentcolor}{ETH7}

\begin{document}

%\def\titlefigure{elements/title-page-image}		% Default image
%\def\titlefigure{elements/title-page-image-43}	% Use this for 4:3 presentations

\titleframe

% \colorlet{titlefgcolor}{ETH5}
% \def\titlefigure{elements/title-page-image-alt}
% \title{Different background}
% \titleframe

% \colorlet{titlefgcolor}{ETH4}
% \colorlet{titlebgcolor}{ETH2}
% \def\titlefigure{}
% \setlength{\titleboxwidth}{0.75\textwidth}			% Change box width
% \title{Delayed deep learning for continuous-time dynamical systems}
% \titleframe

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 1 Motivation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]
    \frametitle{Motivation}
    Learn time evolution of continuous-time dynamical system in presence of non-Markovian effects (time-delays/ hysteresis $\to$ partially observed ODE/PDE) using Neural ODEs.\\\vspace{0.1cm}
    \textbf{Problem:} $\dot{x}(t) = f(x(t))$ Markovian!
    \begin{itemize}
        \item discrete-time
            \hspace{5cm}\raisebox{-0.25cm}{\begin{tikzpicture}[line width=1.pt]
                \node[state, text=white, draw=none, fill=gray!50!black, text width=0.6cm, text centered] (t+1)  {$x_{t+1}$};
                \node[state, text=white, draw=none, fill=gray!50!black,left=of t+1, text width=0.6cm, text centered] (t)  {$x_{t}$};
                \node[state, text=white, draw=none, fill=gray!50!black,left=of t, text width=0.6cm, text centered] (t-1)  {$x_{t-K}$};
                \path (t) -- node[auto=false]{\ldots} (t-1);



                \draw[every loop]
                    (t) edge[] node {} (t+1)
                    (t-1) edge[bend left] node {} (t+1);

             \end{tikzpicture}}
             \begin{flalign*}
                 &\Delta x_{t+1} = f(\underbrace{x_t, x_{t-1}, ..., x_{t-K}}_{z_t},\theta)&
             \end{flalign*}
        \vspace{0.25cm}

         \item continuous-time?
             \begin{flalign*}
                 &\dot{x}(t) = f(\textcolor{red}{x_t}), \quad x_t(s) := x(t+s),\quad s \in [-r, 0]\quad &\\
                 &\;\text{\textcolor{red}{$\to\; x_t$ is $\infty$-dimensional}}&
             \end{flalign*}
     \end{itemize}
     \textbf{Idea:} \\Use \textbf{neural delay differential equation} \hspace{0.5cm} $\dot{x}(t) = f(x(t),x(t-\tau),...,x(t-K\cdot \tau),\theta)$\\
    \begin{picture}(50,50) \put(230,80){\hbox{\includegraphics[scale=0.2]{figures/cont_history.png}}} \end{picture} 
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 2 Neural ODEs
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]
    \frametitle{Neural ODEs}
    \textbf{NODE}\\
    \begin{itemize}
        \item $\dot{x}(t) = f(x(t), \theta), \; x(0)=x_0\in \R^n$\; with some loss $J(\theta) = L(x(T)) + \int_0^T \ell(x(t))\,dt$\\
        $\to$ e.g. $J = \sum_{k=1}^N \norm{x(t_k)-x_k^{\text{data}}}_2^2$
        \item Euler discretization:\; $x_{t+1} = f(x_t, \theta) + x_t \;\to \Phi:x_0\mapsto x(T)$  continuous ResNet
        \item Loss gradients:
            \begin{itemize}
                \item adjoint method:\quad $\dfrac{dJ}{d\theta} = \int_{0}^{T} \lambda(t)^\top \dfrac{\partial f(x(t),\theta)}{\partial \theta}\,dt$\\\vspace{0.1cm}
                with $\dot{\lambda}(t)^\top = -\ell'(x(t)) - \lambda(t)^\top \dfrac{\partial f(x(t),\theta)}{\partial x(t)},\quad \lambda(T)=L'(x(T))$\vspace{0.1cm}
                
                \item discrete sensitivities $\to$ automatic differentiation through solver
            \end{itemize}
     \end{itemize}
     \textbf{ANODE}\\
     \begin{itemize}
         \item Augment state dimension $z(t):=\left(x(t),z_{\text{aug}}(t)\right)^\top\in\R^{n+m}$ and learn NODE in higher dimensional state
         \item For $m\geq n$: Universal approximation of homeomorphisms $H:\R^n\to \R^n$
     \end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 3 Neural NDDEs
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]
    \frametitle{Our model}
        \begin{block}{Neural Delay Differential Equation (NDDE)}
        $\dot{x}(t) = f(x(t), x(t-\tau), ..., x(t-K\cdot\tau), \theta), \;\text{ with } x(t) = \psi(t) \text{ for } t\in [-r,0], r:=K\tau $
    \end{block}
    \begin{itemize}
        \item Need a DDE solver with interpolation
        \item Use $(n(K+1), 32, 64, 128, 64, 32, n)$ NN with swish activation for $f(\cdot,\theta)$
        \item Euler discretization:\; $x_{t+1} = f(x_t, x_{t-1},..., x_{t-K}, \theta) + x_t$\\
        $\to$ combination of ResNet- and DenseNet-like connections
        \item Loss gradients: $J(\theta) = L(x(T)) + \int_0^T \ell(x(t))\,dt$
        \begin{itemize}
            \item adjoint method:\quad $\dfrac{dJ}{d\theta} = \int_0^T \left[\lambda^\top\partial_\theta f+\partial_\theta \ell \right]\, dt$\\\vspace{0.1cm}
            $\begin{cases}\dot{\lambda}(t)^\top = -\ell'(x(t)) - \lambda(t)^\top \dfrac{\partial f(x(t),x(t-\tau),\theta)}{\partial x(t)} - \lambda(t+\tau)^\top \dfrac{\partial f(x(t+\tau),x(t),\theta)}{\partial x(t)},\\\lambda(T)=L'(x(T))\end{cases}$\vspace{0.1cm}
            \item discrete sensitivities $\to$ automatic differentiation through solver
        \end{itemize}
     \end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 5 Partially observed oscillator
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]
    \frametitle{Partially observed oscillator $\ddot{x}(t) + x(t) = 0, x(0)=1, \dot{x}(0) = 0$}
    \begin{columns}[T]
        \begin{column}{0.25\textwidth}
            \vspace{-0.5cm}
            \begin{figure}
                \centering
                \includegraphics[width=0.94\columnwidth]{figures/vanilla_node.png}
                            \vspace{-0.2cm}
                \caption{Vanilla NODE}
            \end{figure}
            \vspace{-0.7cm}
            \begin{figure}
                \centering
                \includegraphics[width=0.94\columnwidth]{figures/ndde.png}                            \vspace{-0.2cm}
                \caption{NDDE $\tau=1, K=1$}
            \end{figure}
        \end{column}
        \begin{column}{0.5\textwidth}
            \vspace{-0.5cm}
            \begin{figure}
                \centering
                \includegraphics[width=0.9\columnwidth]{figures/anode_true_init.png}                            \vspace{-0.2cm}
                \caption{ANODE true initial condition}
            \end{figure}
            \vspace{-0.7cm}
            \begin{figure}
                \centering
                \includegraphics[width=0.9\columnwidth]{figures/anode_learn_init.png}
                                            \vspace{-0.2cm}
                \caption{ANODE learn initial condition}
            \end{figure}
        \end{column}
        \begin{column}[T]{0.25\textwidth}
            \textbf{ANODE:}
            \begin{itemize}
                \item initialization of augmented states problematic
            \end{itemize}
            \textbf{NDDE:}
            \begin{itemize}
                \item works well with observation noise
                \item generalization over time and initial conditions
                \item also on more complex data
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 4 Expressivity
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]
    \frametitle{Theoretical insights on expressivity}
    \textbf{Universal approximation theorem for input-output-map}\\\vspace{0.1cm}
    For any $H:\R^n \to\R^n$ there is a DDE with a single delay such that $\Phi(\psi_{x_0}) = x(T) = H(x_0)$ where $\psi_{x_0}(s)=x_0$.\\
    \vspace{0.25cm}
    \textbf{Approximation along trajectory}
    \begin{itemize}
        \item Can approximate truncated Fourier series as solutions of linear DDEs
        \item Approximation of $1-$dimensional observations of periodic and chaotic attractors (Takens Embedding Theorem)
     \end{itemize}
     \begin{figure}
                \centering
                \includegraphics[width=0.5\textwidth]{figures/lorenz.png}
            \end{figure}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 7 Stability definition
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]
    \frametitle{Exponential stability of time delay systems}
    \begin{definition}
    The time delay system $\dot{x}(t) = f(x_t)$ is said to be exponentially stable if there exist $M>0, \gamma>0$ such that for any initial history $x_{t_0} = \psi$ it holds,
    $$\norm{x(t)}_2 \leq e^{-\gamma (t-t_0)}M\norm{\psi}_\infty \quad \text{for } t\geq t_0.$$
    \end{definition}
    \textbf{Example: } $\dot{x}(t) = -x(t-\tau) \to$ exponentially stable for $\tau<\pi/2$.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 8 Stability
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]
    \frametitle{Non-linear stability analysis} 
    \textbf{non-delayed case} \; $\dot{x}(t) = f(x(t))$
        \begin{itemize}
            \item Lyapunov functions (for exponential stability)
                \begin{itemize}
                    \item $c_1\norm{x}_2^p \leq V(x) \leq c_2\norm{x}_2^p$
                    \item $\dot{V}\left(x(t)\right)\leq -\alpha V(x(t))$ \hspace{1.5cm} $\to$ hard to find
                \end{itemize}
            \item ML approach: Learn a neural network Lyapunov function $V_\phi$
        \end{itemize}
        \vspace{0.5cm}
    \textbf{time delay systems} \; $\dot{x}(t) = f(x_t),\; x_t(s)=x(t+s),\; s\in[-r,0]$
        \begin{itemize}
            \item Lyapunov-Krasovskii functionals\\
            $\to$ natural extension of Lyapunov functions to time delay systems
            \item Lyapunov-Razumikhin functions\\
            $\to$ use Lyapunov functions, but relax decay condition
            
        \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 9 Razumikhin Theorem
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]
    \frametitle{Lyapunov-Razumikhin functions (exponential stability)} 
    \begin{theorem}[Razumikhin]
    Assume there exists a differentiable function \\$V:\R\times\R^n\to \R_+$ and constants $q>1, c_1>0, c_2>0, \alpha>0$ such that the following hold:
    \begin{enumerate}[(i)]
        \item $c_1 \norm{x}_2^2 \leq V(x) \leq c_2 \norm{x}_2^2$
        \item $\dot{V}(x(t)) \leq -\alpha V(x(t))$\\
        whenever $V(x(t+s))\leq q V(x(t)) \quad\forall s\in [-r,0]$.
    \end{enumerate}
    Then $\dot{x}(t) = f(x_t)$ is exponentially stable with decay rate $\gamma = \min(\alpha, \frac{\log q}{r})/2$.
\end{theorem}
\textbf{Intuition:} Require decay condition only when we are about to leave the sublevel set \;$V^{\leq \eta}$ of \linebreak $\eta := \sup_{s\in [-r,0]} V(x(t+s))$.


\begin{picture}(50,50) \put(110,-20){\hbox{\includegraphics[scale=0.2]{figures/sublevelset.png}}}
\end{picture} 
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 10 Discretization of Razumikhin Condition
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]
    \frametitle{Discretization of Razumikhin Condition} 
Let $K$ and $\tau$ be such that $r_V:=K\tau\geq r$ and let $x(\cdot)$ be a solution of $\dot{x}(t)=f(x_t)$. Assume $f$ to be $L_f$-Lipschitz with respect to $\norm{\cdot}_\infty$ and $V:\R^n\to\R_+$ to be convex with,
$$c_1 \norm{x}_2^2 \leq V(x) \leq c_2 \norm{x}_2^2.$$ Furthermore, assume that either of the following conditions is satisfied:
\begin{itemize}
    \item  $\max_{s\in[t_0-r_V,t_0]} \norm{x(s)}_2 \geq \varepsilon$
    \textcolor{red}{$\to$ show convergence to $\varepsilon-$neighbourhood of origin only}
    \item $\max_{s\in[t_0-r_V,t_0]} \norm{x(s)}_2 \leq g(c)\max_{s\in[t_0-r_V+c,t_0+c]}\norm{x(s)}_2$ \\\vspace{0.1cm}
    \textcolor{red}{$\to$ satisfied for exponentially decaying oscillations (if $r_V$ large enough)}
\end{itemize}
Then if,
\begin{equation}
    V(x(t_0-k\tau))\leq q V(x(t_0)) \quad,\forall k\in \lbrace 0,1,...,K \rbrace,\label{eq:DiscRazConProp}
\end{equation}
and $\tau$ is small enough, it holds for any $s\in[-r_V,0]$,
\begin{equation*}
    V(x(t_0+s))\leq \tilde{q}(\tau)V(x(t_0)),
    \text{ with } \tilde{q}(\tau)= q+\mathcal{O}(\tau^2) \text{ as } \tau\to 0.
\end{equation*}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 11 Soft-stability enforcement
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]
    \frametitle{Soft-stability enforcement}
    \textbf{Dynamics:}  $\dot{x}(t)=f_\theta(\V{x_\tau}(t))$, \;where $\V{x_\tau}(t) = \myvec{x(t),x(t-\tau),...,x(t-K\cdot\tau)}$
    \begin{block}{Loss}
        $\ell\left(\V{x_\tau}(t)\right) = \text{ReLU}\left(\dot{V}_{\phi,\theta}\left(\V{x_\tau}(t)\right)+\alpha V_\phi(x(t))\right) R_\phi\left(\V{x_\tau}(t)\right)$
    \end{block}
    \begin{itemize}
        \item $V_\phi(x(t))$ based on input convex neural network and such that,
            \begin{itemize}
                \item $V$ is convex and $V\in\mathcal{C}^2$
                \item $c_1 \norm{x}_2^2 \leq V(x) \leq c_2 \norm{x}_2^2$
            \end{itemize}
        \item $\dot{V}_{\phi,\theta}\left(\V{x_\tau}(t)\right) = \nabla V_\phi(x(t))^\top f_\theta \left(\V{x_\tau}(t)\right)$
        \item $R(\V{x_\tau}(t)) := \prod_{k=1}^K \Theta\left(q V(x(t))-V(x(t-k\cdot\tau))\right)$, \; $\Theta$ unit step function
    \end{itemize}
    $\to$ If $\ell\left(\V{x_\tau}(t)\right)$ is zero along a trajectory, then this trajectory converges exponentially towards\\ \hspace{0.45cm}the origin.
    
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 12 Training
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]
    \frametitle{Training}
    \textbf{Along trajectory}\\
        Minimize continuous loss along solution.
        \begin{flalign*}
            J(\phi,\theta) &= \int_0^T \ell_d\left(\V{x_\tau}(t)\right) dt\\
            &= \int_0^T \textcolor{red}{\sigma_d}\left(\dot{V}_{\phi,\theta}\left(\V{x_\tau}(t)\right)+\alpha V_\phi(x(t))\right) \prod_{k=1}^K \textcolor{red}{\Theta_d}\left(q V(x(t))-V(x(t-k\cdot\tau))\right)dt& 
        \end{flalign*}
        $\to$ need \textcolor{red}{smoothed step} and \textcolor{red}{ReLU} for DDE solver\\
    \vspace{0.25cm}
    \textbf{Uncorrelated}\\
    Create data set $\lbrace \V{x}^{(i)} \rbrace_{i=1}^N$ of samples $ \V{x}^{(i)}\in \R^{n(K+1)}$ and minimize $\frac{1}{N}\sum_{i=1}^N \ell\left(\V{x}^{(i)}\right)$\\
    \vspace{0.25cm}
    $\to$ no need for smoothing, but more conservative
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 13 Proof of concept
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]
    \frametitle{Proof of concept}
    \textbf{Inverted pendulum with delayed feedback}\quad $x(t)=\left(\varphi(t), \dot{\varphi}(t)\right)^\top$\\
    \begin{flalign*}
        &\dot{x}(t) = \myvec{\dot{x_1}(t)\\\dot{x_2}(t)} = \myvec{x_2(t)\\\dfrac{g}{l} \sin(x_1(t)) - \dfrac{b}{ml^2}x_2(t) + \dfrac{1}{ml^2}u(t-\tau)}&
    \end{flalign*}
    \textbf{Goal} \;\\Learn linear stabilizing feedback\; $u(t) = \pi(x(t)) = k_1 x_1(t) + k_2 x_2(t)$\\
    $\to $ initialize with LQR feedback\\
    \vspace{0.25cm}
    \textbf{Setup}\vspace{-0.5cm}
    \begin{center}\begin{tabular}{  c | c | c | c |c  }
          $l$ & $m$ & $b$ & $g$ & $\tau$ \\
         \hline
           0.3 & 0.2 & 0 & 9.81 &0.03
        \end{tabular}\end{center}
    \begin{tabular}{ l | c | c | c | c |c | c |c |c| c  }
 training & $\tau_V$ & $K_V$& $r_V $  & $\alpha$ & $q$ &$T$ & $d_L$ & data size & batch size\\
 \hline
  along trajectory& 0.04 & 5 & 0.2 & 0.1 & 1.0202 & 1.5& 0.5& - & - \\
  uncorrelated& 0.04 & 5 & 0.2 & 0.1 & 1.0202 & - & - & 64000 & 64
\end{tabular}
\begin{picture}(50,50) \put(285,95){\hbox{\includegraphics[scale=0.15]{figures/LQR.png}}}
\end{picture} 
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 14 Stabilization experiments
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
    \frametitle{Stabilization experiments}
    \begin{columns}
        \begin{column}{0.3\textwidth}
            \vspace{-1cm}
            \begin{figure}[h!]
                 \centering
                 \includegraphics[width=\columnwidth]{figures/nn_cont/extrapolation10.png}
             \end{figure}
             \vspace{-1cm}
             \begin{figure}[h!]
                 \centering
                 \includegraphics[width=\columnwidth]{figures/nn_cont/V_trained.png}
                 \caption{Along single trajectory}
             \end{figure}
        \end{column}
        \begin{column}{0.3\textwidth}
        \vspace{-1cm}
            \begin{figure}[h!]
                \centering
                \includegraphics[width=\columnwidth]{figures/nn_disc/extrapolation10.png}
            \end{figure}
            \vspace{-1cm}
            \begin{figure}[h!]
                 \centering
                 \includegraphics[width=\columnwidth]{figures/nn_disc/V_nn_trained_uncorrelated.png}
                 \caption{Uncorrelated}
             \end{figure}
        \end{column}
        \begin{column}{0.4\textwidth}
        \textbf{}
        \textbf{Generalization}\\
        Evaluate continuous loss along 8 trajectories starting from
        $x(0)=(\cos(2\pi l/8), \sin(2\pi l/8))_{l=1}^8$
            \begin{center}
                \begin{tabular}{ l | c }
 method& test loss\\
 \hline
  along trajectory& 3.1502\\
  uncorrelated& 0.0000
\end{tabular}
            \end{center}
        \end{column}
    \end{columns}
\end{frame}


\begin{closingframe}

Professor John Doe\\
Role of person giving presentation\\
\email{beat.muster@abcd.ethz.ch}

\medskip

ETH Zurich\\
Organisational unit\\
Building Room\\
Street House number\\
0000 Town, Country\\
\url{http://www.abcd.ethz.ch}

\end{closingframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bonus slides:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% B1 Discretization of Razumikhin Condition
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]
    \frametitle{Discretization of Razumikhin Condition} 
Let $K$ and $\tau$ be such that $r_V:=K\tau\geq r$ and let $x(\cdot)$ be a solution of $\dot{x}(t)=f(x_t)$ passing through $x_{t_0}\in\mathcal{C}([-r,0])$. Let $\rho:=\max_{s\in[t_0-r_V,t_0]} \norm{x(s)}_2<\infty$ and assume $f$ to be $L_f$-Lipschitz with respect to $\norm{\cdot}_\infty$. Moreover, let $V:\R^n\to\R_+$ be convex with,
$$c_1 \norm{x}_2^2 \leq V(x) \leq c_2 \norm{x}_2^2,$$
$f$ and $V$ both be differentiable, and assume that there is a function $g:\R_+\to\R_+$ with $g(s)\geq 1$ such that along the trajectory it holds for any $t$ and $c>0$,
\begin{equation}
    \max_{s\in[t-r_V,t]} \norm{x(s)}_2 \leq g(c) \max_{s\in[t-r_V+c,t+c]}\norm{x(s)}_2.\label{eq:small_decay_cond}
\end{equation}
Now if,
\begin{equation}
    V(x(t_0-k\tau))\leq q V(x(t_0)) \quad,\forall k\in \lbrace 0,1,...,K \rbrace,\label{eq:DiscRazConProp}
\end{equation}
and $8c_1 > (4c_2-c_1)L_f^2 g(2r) \tau^2$, then it holds for any $s\in[-r_V,0]$,
\begin{align}
    V(x(t_0+s))&\leq \tilde{q}(\tau)V(x(t_0)),\label{eq:ContRazConProp}\\
    \text{with } \tilde{q}(\tau)&= \dfrac{q}{1-(4c_2-c_1)L_f^2g(2r)\tau^2/(8c_1)}=q+\mathcal{O}(\tau^2).\nonumber
\end{align}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% B2 Noisy observations
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
    \frametitle{Noisy observations}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{figure}[h!]
                 \centering
                 \includegraphics[width=\columnwidth]{figures/10d05sigma_smoothed.png}
                 \caption{Noise $\sigma=0.5$}
             \end{figure}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{figure}[h!]
                \centering
                \includegraphics[width=\columnwidth]{figures/10d05sigma_conf.png}
                \caption{$K=10, \sigma=0.5$}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% B3 Partially observed oscillator -> delete this
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]
    \frametitle{Partially observed oscillator $\ddot{x}(t) + x(t) = 0, x(0)=1, \dot{x}(0) = 0$}
    \begin{columns}
        \begin{column}{0.25\textwidth}
            \begin{figure}
                \centering
                \includegraphics[width=\columnwidth]{figures/vanilla_node.png}
                \caption{Vanilla NODE}
            \end{figure}
        \end{column}
        \begin{column}{0.5\textwidth}
            \vspace{-0.3cm}
            \begin{figure}
                \centering
                \includegraphics[width=0.9\columnwidth]{figures/anode_true_init.png}
                \caption{ANODE true initial condition}
            \end{figure}
            \vspace{-0.7cm}
            \begin{figure}
                \centering
                \includegraphics[width=0.9\columnwidth]{figures/anode_learn_init.png}
                \caption{ANODE learn initial condition}
            \end{figure}
        \end{column}
        \begin{column}{0.25\textwidth}
            \begin{figure}
                \centering
                \includegraphics[width=\columnwidth]{figures/ndde.png}
                \caption{NDDE}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}

\end{document}